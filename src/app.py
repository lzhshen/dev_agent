import os

import streamlit as st
from streamlit.logger import get_logger
from langchain_core.messages import AIMessage, HumanMessage
from langchain_openai import ChatOpenAI
from langchain_community.llms import Tongyi
from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from streamlit_float import *
from langchain.agents import create_tool_calling_agent
from langchain.agents import initialize_agent, load_tools

import database
import view
from utils import *

user_story_template = """
You are a business analyst who is familiar with specification by example. Iâ€™m the domain expert.
// ä½ æ˜¯ä¸€ä¸ªä¸šåŠ¡åˆ†æå¸ˆï¼Œè€Œæˆ‘æ˜¯é¢†åŸŸä¸“å®¶ 

===CONTEXT
{context}
===END OF CONTEXT

===USER STORY
{story}
===END OF USER STORY 

Explain the user story as scenarios. Use the following format:
// ä½¿ç”¨ åœºæ™¯ è§£é‡Šç”¨æˆ·æ•…äº‹ï¼Œå¹¶éµå¾ªå¦‚ä¸‹æ ¼å¼ 

Thought: you should always think about what is still uncertain about the user story. Ignore technical concerns.
// æ€è€ƒï¼šä½ åº”è¯¥è€ƒè™‘ç”¨æˆ·æ•…äº‹ä¸­ä¸æ¸…æ™°çš„éƒ¨åˆ†ã€‚ä½†å¿½ç•¥æŠ€æœ¯ç»†èŠ‚
Question: the question to ask to clarify the user story
// é—®é¢˜ï¼šæå‡ºé—®é¢˜å¸®åŠ©ä½ æ¾„æ¸…è¿™ä¸ªç”¨æˆ·æ•…äº‹
Answer: the answer I responded to the question
// å›ç­”ï¼šæˆ‘ç»™å‡ºç­”æ¡ˆ
â€¦ (this Thought/Question/Answer repeat at least 3 times, at most 10 times)
//ï¼ˆThought/Question/Answer é‡å¤è‡³å°‘ 3 æ¬¡è€Œä¸å¤šäº 10 æ¬¡ï¼‰
Thought: I know enough to explain the user story
// æ€è€ƒï¼šæˆ‘å·²ç»å¯¹è¿™ä¸ªç”¨æˆ·æ•…äº‹äº†è§£äº†è¶³å¤Ÿå¤šçš„å†…å®¹
Scenarios: List all possible scenarios with concrete example in Given/When/Then style
// åœºæ™¯ï¼šåˆ—å‡ºæ‰€æœ‰åœºæ™¯ã€‚ä½¿ç”¨ Given/When/Then çš„æ ¼å¼è¡¨è¿°

å¯¹äºæ¯ä¸ªæ­¥éª¤ï¼ˆThought/Question/Answer/Scenariosï¼‰è¯·æ¢è¡Œè¾“å‡ºå…¶å†…å®¹

{history}
{input}
è¯·ä½¿ç”¨ä¸­æ–‡
"""
# app config
st.set_page_config(page_title="Streaming bot", page_icon="ğŸ¤–", layout="wide")
st.title("Streaming bot")

log = get_logger(__name__)
log.info("###################### st.rerun ######################")

float_init(theme=True, include_unstable_primary=False)

load_dotenv()

# `set_page_config()` must be called as the first Streamlit command in your script.
database.init_database()


def get_response(user_query, chat_history, user_story, business_ctx, is_interactive=True):
    if "DASHSCOPE_API_KEY" in os.environ:
        llm_chat = Tongyi
        llm_model_name = "qwen1.5-0.5b-chat"  # é€šä¹‰åƒé—®1.5å¯¹å¤–å¼€æºçš„0.5Bè§„æ¨¡å‚æ•°é‡æ˜¯ç»è¿‡äººç±»æŒ‡ä»¤å¯¹é½çš„chatæ¨¡å‹
        # llm_model_name = "qwen1.5-110b-chat"  # é€šä¹‰åƒé—®1.5å¯¹å¤–å¼€æºçš„110Bè§„æ¨¡å‚æ•°é‡æ˜¯ç»è¿‡äººç±»æŒ‡ä»¤å¯¹é½çš„chatæ¨¡å‹
        # llm_model_name = "baichuan-7b-v1"  # ç”±ç™¾å·æ™ºèƒ½å¼€å‘çš„ä¸€ä¸ªå¼€æºçš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼Œ70äº¿å‚æ•°ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ï¼Œä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸º4096ã€‚
        # llm_model_name = "baichuan2-13b-chat-v1"  # ç”±ç™¾å·æ™ºèƒ½å¼€å‘çš„ä¸€ä¸ªå¼€æºçš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼Œ130äº¿å‚æ•°ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ï¼Œä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸º4096ã€‚
        # llm_model_name = "llama3-8b-instruct"  # Llama3ç³»åˆ—æ˜¯Metaåœ¨2024å¹´4æœˆ18æ—¥å…¬å¼€å‘å¸ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œllama3-8Bæ‹¥æœ‰80äº¿å‚æ•°ï¼Œæ¨¡å‹æœ€å¤§è¾“å…¥ä¸º6500ï¼Œæœ€å¤§è¾“å‡ºä¸º1500ï¼Œä»…æ”¯æŒmessageæ ¼å¼ï¼Œé™æ—¶å…è´¹è°ƒç”¨ã€‚
        # llm_model_name = "ziya-llama-13b-v1"  # å§œå­ç‰™é€šç”¨å¤§æ¨¡å‹ç”±IDEAç ”ç©¶é™¢è®¤çŸ¥è®¡ç®—ä¸è‡ªç„¶è¯­è¨€ç ”ç©¶ä¸­å¿ƒä¸»å¯¼å¼€æºï¼Œå…·å¤‡ç¿»è¯‘ã€ç¼–ç¨‹ã€æ–‡æœ¬åˆ†ç±»ã€ä¿¡æ¯æŠ½å–ã€æ‘˜è¦ã€æ–‡æ¡ˆç”Ÿæˆã€å¸¸è¯†é—®ç­”å’Œæ•°å­¦è®¡ç®—ç­‰èƒ½åŠ›ã€‚
        # llm_model_name = "chatyuan-large-v2"  # ChatYuanæ¨¡å‹æ˜¯ç”±å…ƒè¯­æ™ºèƒ½å‡ºå“çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œå®ƒåœ¨çµç§¯å¹³å°ä¸Šçš„æ¨¡å‹åç§°ä¸º"chatyuan-large-v2"ã€‚ChatYuan-large-v2æ˜¯ä¸€ä¸ªæ”¯æŒä¸­è‹±åŒè¯­çš„åŠŸèƒ½å‹å¯¹è¯è¯­è¨€å¤§æ¨¡å‹ï¼Œæ˜¯ç»§ChatYuanç³»åˆ—ä¸­ChatYuan-large-v1å¼€æºåçš„åˆä¸€ä¸ªå¼€æºæ¨¡å‹ã€‚

    # elif "OPENAI_API_KEY" in os.environ:
    else:
        llm_chat = ChatOpenAI
        llm_model_name = "gpt-4-turbo-preview"

    if is_interactive:
        llm = llm_chat(temperature=0.0, model=llm_model_name, model_kwargs={"stop": "\nAnswer"})
    else:
        llm = llm_chat(temperature=0.0, model=llm_model_name)
    # output_parser = StrOutputParser()
    output_parser = MyStrOutputParser()
    prompt = ChatPromptTemplate.from_template(user_story_template)
    chain = prompt | llm | output_parser

    stream = chain.stream(
        {
            "input": user_query,
            "history": chat_history,
            "story": user_story,
            "context": business_ctx,
        }
    )
    return stream


# Initialize chat history
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
    border = False
else:
    border = True

# session state
if "chat_history" not in st.session_state:
    st.session_state.chat_history = [
        AIMessage(content="Hello, I am a bot. How can I help you?"),
    ]
    border = False
else:
    border = True

left_column, right_column = st.columns(2)
with right_column:
    # tab_user_story, tab_ddd, tab_tdd = st.tabs(["User Story", "DDD", "TDD"])
    # with tab_user_story:
    user_story, business_ctx = view.user_story_tab()

with left_column:
    with st.container(border=border, height=1100):
        # conversation
        for message in st.session_state.chat_history:
            if isinstance(message, AIMessage):
                with st.chat_message("AI"):
                    st.write(message.content)
            elif isinstance(message, HumanMessage):
                with st.chat_message("Human"):
                    st.write(message.content)

        # user input
        user_query = ''
        with st.container():
            is_interactive = st.checkbox("äº¤äº’å¯¹è¯æ¨¡å¼", value=False)

            user_query = st.chat_input("What is up?")
            button_b_pos = "0rem"
            button_css = float_css_helper(width="2.2rem", bottom=button_b_pos, transition=0)
            float_parent(css=button_css)

        if user_query is not None and user_query != "":
            st.session_state.chat_history.append(HumanMessage(content=user_query))

            with st.chat_message("Human"):
                st.markdown(user_query)

            with st.chat_message("AI"):
                # response = st.write_stream(get_response(user_query, st.session_state.chat_history, right_column.user_story, right_column.business_ctx))
                response = st.write_stream(get_response(user_query, st.session_state.chat_history, user_story, business_ctx, is_interactive))

            st.session_state.chat_history.append(AIMessage(content=response))

